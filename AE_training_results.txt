Autoencoders 10% (a):
	num_epochs = 200
	patience = 10
	embedding_dim = 24
	batch_size = 128
	learning_rate = 0.001
	seq_len = 100
	Loss: 0.0032
	Validation Loss: 0.0152

Autoencoders 10% (b):
	num_epochs = 200
	patience = 10
	dropout_rate = 0.2
	embedding_dim = 24
	batch_size = 128
	learning_rate = 0.001
	seq_len = 100
	Loss: 0.0093
	Validation Loss: 0.0162

Autoencoders 1 layer 10% (c):
	num_epochs = 600
	patience = 50
	dropout_rate = 0.1
	embedding_dim = 40
	batch_size = 32
	learning_rate = 0.001
	seq_len = 10
	Loss: 0.0028
	Validation Loss: 0.0092

Autoencoders 20% (d):
	num_epochs = 200
	patience = 10
	embedding_dim = 24
	batch_size = 128
	learning_rate = 0.001
	seq_len = 100
	Loss: 0.0079
	Validation Loss: 0.0168

Autoencoders 20% (e):
	num_epochs = 200
	patience = 10
	dropout_rate = 0.2
	embedding_dim = 24
	batch_size = 128
	learning_rate = 0.001
	seq_len = 100
	Loss: 0.0084
	Validation Loss: 0.0141

Autoencoders 30% (f):
	num_epochs = 200
	patience = 10
	dropout_rate = 0.2
	embedding_dim = 24
	batch_size = 128
	learning_rate = 0.001
	seq_len = 100
	Loss: 0.0113
	Validation Loss: 0.0186

Autoencoders 50% (g):
	num_epochs = 200
	patience = 10
	dropout_rate = 0.2
	embedding_dim = 24
	batch_size = 128
	learning_rate = 0.001
	seq_len = 100
	Loss: 0.0391
	Validation Loss: 0.0465

Autoencoders 50% (h):
	num_epochs = 200
	patience = 10
	dropout_rate = 0
	embedding_dim = 24
	batch_size = 128
	learning_rate = 0.001
	seq_len = 100
	Loss: 0.0146
	Validation Loss: 0.0182

VAE Autoencoders 10% (i):
	num_epochs = 200
	patience = 10
	dropout_rate = 0
	embedding_dim = 24
	batch_size = 128
	learning_rate = 0.001
	seq_len = 100
	Epoch [120/200], Loss: 0.0402
	Validation Loss: 0.0462

VAE Autoencoders 1 layer 10% (l):
	num_epochs = 200
	patience = 10
	dropout_rate = 0.1
	embedding_dim = 24
	batch_size = 32
	learning_rate = 0.001
	seq_len = 100
	Loss: 0.0278
	Validation Loss: 0.0430
